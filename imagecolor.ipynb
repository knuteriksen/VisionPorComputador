{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imagecolor.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3acROV8DpFwz",
        "hEuR-tkzqJ5j",
        "479DvEYdrX5v",
        "ueFftUjKrgNV",
        "y-JQ-rEYrjmP"
      ],
      "authorship_tag": "ABX9TyOCLAIwz0pCPAjM/XCyk6n0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/knuteriksen/VisionPorComputador/blob/main/imagecolor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2erBPnfFYhLt"
      },
      "source": [
        "# Image Colorization\n",
        "\n",
        "Thanks to [Emil Wallner](https://blog.floydhub.com/colorizing-b-w-photos-with-neural-networks/) for inspiration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Jaf0C4RbZnx"
      },
      "source": [
        "# Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDteTNfeD_Oh"
      },
      "source": [
        "import keras\n",
        "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
        "from keras.preprocessing import image\n",
        "from keras.layers import Layer\n",
        "from keras.applications.inception_resnet_v2 import preprocess_input\n",
        "from keras.layers import Conv2D, UpSampling2D, InputLayer, Conv2DTranspose, Input, Reshape, merge, concatenate, Activation, Dense, Dropout, Flatten\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.callbacks import TensorBoard \n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers.core import RepeatVector, Permute\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\n",
        "from skimage.transform import resize\n",
        "from skimage.io import imsave\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jugVjBQvZGQ7"
      },
      "source": [
        "# Initialization\n",
        "Make sure that all keras stuff from previous sessions is cleared, and make sure we run in Eagerly mode. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iujoOigC5IEw"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "tf.config.run_functions_eagerly(True)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5XACkmnbjZQ"
      },
      "source": [
        "# Obtaining images\n",
        "\n",
        "Get the training images from the local repository `Train/` and convert them to an array float type representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwDaIu5sY7JQ"
      },
      "source": [
        "# Get images\n",
        "X = []\n",
        "for filename in os.listdir('Train/'):\n",
        "    X.append(img_to_array(load_img('Train/'+filename)))\n",
        "X = np.array(X, dtype=float)\n",
        "Xtrain = 1.0/255*X"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPZmf5Etblru"
      },
      "source": [
        "# Downloading the Inception ResNet V2 with imagenet weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hry2uzQ5Y9JS"
      },
      "source": [
        "inception = InceptionResNetV2(weights='imagenet', include_top=True)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrAA3wJ0ch_3"
      },
      "source": [
        "# Create the Encoder\n",
        "\n",
        "Just a basic convoloutional encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9By_7TkeZSVs"
      },
      "source": [
        "encoder_input = Input(shape=(256, 256, 1,))\n",
        "encoder_output = Conv2D(64, (3,3), activation='relu', padding='same', strides=2)(encoder_input)\n",
        "encoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(encoder_output)\n",
        "encoder_output = Conv2D(128, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
        "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
        "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
        "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
        "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
        "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06EpjAKhckvO"
      },
      "source": [
        "# Create the fusion layer\n",
        "The fusion layer takes as input both the output of the encoder and the output of the inception layer.\n",
        "\n",
        "First Create an `Input` type. This variable will represent the output of the inception layer.\n",
        "\n",
        "Then:\n",
        "\n",
        "1.   Multiply the 1000 category Input layer `embed_input` (which contains the output of the inception layer) by 32*32 and store it in the `fusion_output` variable\n",
        "2.   Then Reshape the `fusion_output` layer\n",
        "3.   Then concatenate the fusion layer (now existing of the reshaped inception output) and the encoder output. **This is the actual fusion/concatination of the encoder and the inception.**\n",
        "4. Then do a 1x1 convolution to obtain a final size of 256@32x32 that we need for the decoder.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHbYdZVYZf-7"
      },
      "source": [
        "embed_input = Input(shape=(1000,))\n",
        "fusion_output = RepeatVector(32 * 32)(embed_input) \n",
        "fusion_output = Reshape(([32, 32, 1000]))(fusion_output)\n",
        "fusion_output = tf.keras.layers.Concatenate(axis=3)([encoder_output, fusion_output])\n",
        "fusion_output = Conv2D(256, (1, 1), activation='relu', padding='same')(fusion_output) "
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kos5zpzcnTO"
      },
      "source": [
        "# Create the Decoder\n",
        "The Decoder takes the output of the fusion layer as input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9N-AboQZjKI"
      },
      "source": [
        "decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(fusion_output)\n",
        "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
        "decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\n",
        "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
        "decoder_output = Conv2D(32, (3,3), activation='relu', padding='same')(decoder_output)\n",
        "decoder_output = Conv2D(16, (3,3), activation='relu', padding='same')(decoder_output)\n",
        "decoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)\n",
        "decoder_output = UpSampling2D((2, 2))(decoder_output)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aE-ZCy-ic790"
      },
      "source": [
        "# Put together the whole model \n",
        "Remember that `embed_input` contains the output of the inception layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7kcRIXZZloJ"
      },
      "source": [
        "model = Model(inputs=[encoder_input, embed_input], outputs=decoder_output)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qmi2HxuudGv1"
      },
      "source": [
        "# Get inception output\n",
        "This output will be used as `embed_input` mentioned above.\n",
        "\n",
        "1.    First resize the grayscaled input images to fit into the inception model\n",
        "2.    Then use the preprocessor to feed the images in the correct format\n",
        "3.    Then we run the input through inception resnet v2 network and extracts the final layer of the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU3IZK19ZozB"
      },
      "source": [
        "def create_inception_embedding(grayscaled_rgb):\n",
        "    grayscaled_rgb_resized = []\n",
        "    for i in grayscaled_rgb:\n",
        "      i = resize(i, (299, 299, 3), mode='constant')\n",
        "      grayscaled_rgb_resized.append(i)\n",
        "    grayscaled_rgb_resized = np.array(grayscaled_rgb_resized)\n",
        "    grayscaled_rgb_resized = preprocess_input(grayscaled_rgb_resized)\n",
        "    embed = inception.predict(grayscaled_rgb_resized)\n",
        "    return embed"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3acROV8DpFwz"
      },
      "source": [
        "# Data Generator\n",
        "This is just  simple image data generator for training purposes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTfIGkJQZoew"
      },
      "source": [
        "datagen = ImageDataGenerator(\n",
        "        shear_range=0.4,\n",
        "        zoom_range=0.4,\n",
        "        rotation_range=40,\n",
        "        horizontal_flip=True)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEuR-tkzqJ5j"
      },
      "source": [
        "# Create Training Data\n",
        "This is the actual generator the backend uses to obtain the correct training data.\n",
        "\n",
        "1.   First we transform the rgb image to a grayscaled rgb\n",
        "2.   Then we use the `create_inception_embedding` funtion to obtain the `embedding_input` metnioned above. Again, this corresponds to the output of the inception layer.\n",
        "3.   Then we get the X and Y training data from the training dataset\n",
        "4.   Then we yield the input corresponding to the model created above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AnL6XTcZoWt"
      },
      "source": [
        "batch_size = 20\n",
        "\n",
        "def image_a_b_gen(batch_size):\n",
        "    for batch in datagen.flow(Xtrain, batch_size=batch_size):\n",
        "        grayscaled_rgb = gray2rgb(rgb2gray(batch))\n",
        "        embed = create_inception_embedding(grayscaled_rgb)\n",
        "        lab_batch = rgb2lab(batch)\n",
        "        X_batch = lab_batch[:,:,:,0]\n",
        "        X_batch = X_batch.reshape(X_batch.shape+(1,))\n",
        "        Y_batch = lab_batch[:,:,:,1:] / 128\n",
        "        yield ([X_batch, embed], Y_batch)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3T1z7LW1rTFe"
      },
      "source": [
        "# Compile and train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yP_mSlejZoMB",
        "outputId": "556290fc-67dc-44c2-a816-657b712b29d9"
      },
      "source": [
        "tensorboard = TensorBoard(log_dir=\"/output\")\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.fit(image_a_b_gen(batch_size), callbacks=[tensorboard], epochs=100, steps_per_epoch=10)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py:3704: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable.debug_mode()`.\n",
            "  \"Even though the `tf.config.experimental_run_functions_eagerly` \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "10/10 [==============================] - 181s 18s/step - loss: 0.1490\n",
            "Epoch 2/100\n",
            "10/10 [==============================] - 165s 17s/step - loss: 0.0065\n",
            "Epoch 3/100\n",
            "10/10 [==============================] - 173s 18s/step - loss: 0.0058\n",
            "Epoch 4/100\n",
            "10/10 [==============================] - 167s 17s/step - loss: 0.0056\n",
            "Epoch 5/100\n",
            "10/10 [==============================] - 152s 15s/step - loss: 0.0057\n",
            "Epoch 6/100\n",
            "10/10 [==============================] - 165s 17s/step - loss: 0.0054\n",
            "Epoch 7/100\n",
            "10/10 [==============================] - 181s 18s/step - loss: 0.0057\n",
            "Epoch 8/100\n",
            "10/10 [==============================] - 188s 19s/step - loss: 0.0058\n",
            "Epoch 9/100\n",
            "10/10 [==============================] - 181s 19s/step - loss: 0.0055\n",
            "Epoch 10/100\n",
            "10/10 [==============================] - 196s 19s/step - loss: 0.0051\n",
            "Epoch 11/100\n",
            "10/10 [==============================] - 189s 19s/step - loss: 0.0051\n",
            "Epoch 12/100\n",
            "10/10 [==============================] - 188s 19s/step - loss: 0.0053\n",
            "Epoch 13/100\n",
            "10/10 [==============================] - 170s 17s/step - loss: 0.0050\n",
            "Epoch 14/100\n",
            "10/10 [==============================] - 154s 16s/step - loss: 0.0049\n",
            "Epoch 15/100\n",
            "10/10 [==============================] - 146s 15s/step - loss: 0.0049\n",
            "Epoch 16/100\n",
            "10/10 [==============================] - 155s 15s/step - loss: 0.0047\n",
            "Epoch 17/100\n",
            "10/10 [==============================] - 188s 19s/step - loss: 0.0047\n",
            "Epoch 18/100\n",
            "10/10 [==============================] - 146s 15s/step - loss: 0.0050\n",
            "Epoch 19/100\n",
            "10/10 [==============================] - 145s 15s/step - loss: 0.0051\n",
            "Epoch 20/100\n",
            "10/10 [==============================] - 180s 19s/step - loss: 0.0045\n",
            "Epoch 21/100\n",
            "10/10 [==============================] - 168s 17s/step - loss: 0.0048\n",
            "Epoch 22/100\n",
            "10/10 [==============================] - 178s 18s/step - loss: 0.0047\n",
            "Epoch 23/100\n",
            "10/10 [==============================] - 207s 21s/step - loss: 0.0044\n",
            "Epoch 24/100\n",
            "10/10 [==============================] - 207s 21s/step - loss: 0.0043\n",
            "Epoch 25/100\n",
            "10/10 [==============================] - 209s 21s/step - loss: 0.0045\n",
            "Epoch 26/100\n",
            "10/10 [==============================] - 185s 18s/step - loss: 0.0045\n",
            "Epoch 27/100\n",
            "10/10 [==============================] - 154s 15s/step - loss: 0.0046\n",
            "Epoch 28/100\n",
            "10/10 [==============================] - 178s 17s/step - loss: 0.0042\n",
            "Epoch 29/100\n",
            "10/10 [==============================] - 146s 14s/step - loss: 0.0041\n",
            "Epoch 30/100\n",
            "10/10 [==============================] - 137s 14s/step - loss: 0.0043\n",
            "Epoch 31/100\n",
            "10/10 [==============================] - 154s 16s/step - loss: 0.0041\n",
            "Epoch 32/100\n",
            "10/10 [==============================] - 154s 15s/step - loss: 0.0043\n",
            "Epoch 33/100\n",
            "10/10 [==============================] - 155s 16s/step - loss: 0.0040\n",
            "Epoch 34/100\n",
            "10/10 [==============================] - 148s 15s/step - loss: 0.0043\n",
            "Epoch 35/100\n",
            "10/10 [==============================] - 188s 18s/step - loss: 0.0042\n",
            "Epoch 36/100\n",
            "10/10 [==============================] - 173s 18s/step - loss: 0.0040\n",
            "Epoch 37/100\n",
            "10/10 [==============================] - 173s 17s/step - loss: 0.0040\n",
            "Epoch 38/100\n",
            "10/10 [==============================] - 164s 16s/step - loss: 0.0039\n",
            "Epoch 39/100\n",
            "10/10 [==============================] - 173s 17s/step - loss: 0.0040\n",
            "Epoch 40/100\n",
            "10/10 [==============================] - 165s 16s/step - loss: 0.0036\n",
            "Epoch 41/100\n",
            "10/10 [==============================] - 180s 19s/step - loss: 0.0036\n",
            "Epoch 42/100\n",
            "10/10 [==============================] - 180s 18s/step - loss: 0.0039\n",
            "Epoch 43/100\n",
            "10/10 [==============================] - 154s 15s/step - loss: 0.0036\n",
            "Epoch 44/100\n",
            "10/10 [==============================] - 145s 15s/step - loss: 0.0034\n",
            "Epoch 45/100\n",
            "10/10 [==============================] - 162s 16s/step - loss: 0.0034\n",
            "Epoch 46/100\n",
            "10/10 [==============================] - 145s 15s/step - loss: 0.0035\n",
            "Epoch 47/100\n",
            "10/10 [==============================] - 167s 16s/step - loss: 0.0032\n",
            "Epoch 48/100\n",
            "10/10 [==============================] - 164s 17s/step - loss: 0.0034\n",
            "Epoch 49/100\n",
            "10/10 [==============================] - 164s 16s/step - loss: 0.0034\n",
            "Epoch 50/100\n",
            "10/10 [==============================] - 146s 14s/step - loss: 0.0031\n",
            "Epoch 51/100\n",
            "10/10 [==============================] - 146s 15s/step - loss: 0.0034\n",
            "Epoch 52/100\n",
            "10/10 [==============================] - 155s 15s/step - loss: 0.0032\n",
            "Epoch 53/100\n",
            "10/10 [==============================] - 155s 16s/step - loss: 0.0030\n",
            "Epoch 54/100\n",
            "10/10 [==============================] - 171s 17s/step - loss: 0.0030\n",
            "Epoch 55/100\n",
            "10/10 [==============================] - 171s 18s/step - loss: 0.0030\n",
            "Epoch 56/100\n",
            "10/10 [==============================] - 163s 17s/step - loss: 0.0029\n",
            "Epoch 57/100\n",
            "10/10 [==============================] - 179s 18s/step - loss: 0.0029\n",
            "Epoch 58/100\n",
            "10/10 [==============================] - 172s 17s/step - loss: 0.0031\n",
            "Epoch 59/100\n",
            "10/10 [==============================] - 181s 19s/step - loss: 0.0027\n",
            "Epoch 60/100\n",
            "10/10 [==============================] - 173s 18s/step - loss: 0.0028\n",
            "Epoch 61/100\n",
            "10/10 [==============================] - 188s 19s/step - loss: 0.0029\n",
            "Epoch 62/100\n",
            "10/10 [==============================] - 189s 19s/step - loss: 0.0029\n",
            "Epoch 63/100\n",
            "10/10 [==============================] - 186s 18s/step - loss: 0.0027\n",
            "Epoch 64/100\n",
            "10/10 [==============================] - 165s 17s/step - loss: 0.0028\n",
            "Epoch 65/100\n",
            "10/10 [==============================] - 167s 16s/step - loss: 0.0026\n",
            "Epoch 66/100\n",
            "10/10 [==============================] - 202s 20s/step - loss: 0.0024\n",
            "Epoch 67/100\n",
            "10/10 [==============================] - 200s 20s/step - loss: 0.0024\n",
            "Epoch 68/100\n",
            "10/10 [==============================] - 192s 19s/step - loss: 0.0024\n",
            "Epoch 69/100\n",
            "10/10 [==============================] - 190s 19s/step - loss: 0.0022\n",
            "Epoch 70/100\n",
            "10/10 [==============================] - 184s 19s/step - loss: 0.0022\n",
            "Epoch 71/100\n",
            "10/10 [==============================] - 174s 18s/step - loss: 0.0021\n",
            "Epoch 72/100\n",
            "10/10 [==============================] - 146s 15s/step - loss: 0.0022\n",
            "Epoch 73/100\n",
            "10/10 [==============================] - 138s 14s/step - loss: 0.0023\n",
            "Epoch 74/100\n",
            "10/10 [==============================] - 163s 17s/step - loss: 0.0024\n",
            "Epoch 75/100\n",
            "10/10 [==============================] - 156s 16s/step - loss: 0.0021\n",
            "Epoch 76/100\n",
            "10/10 [==============================] - 148s 15s/step - loss: 0.0021\n",
            "Epoch 77/100\n",
            "10/10 [==============================] - 156s 16s/step - loss: 0.0021\n",
            "Epoch 78/100\n",
            "10/10 [==============================] - 145s 14s/step - loss: 0.0021\n",
            "Epoch 79/100\n",
            "10/10 [==============================] - 147s 15s/step - loss: 0.0021\n",
            "Epoch 80/100\n",
            "10/10 [==============================] - 147s 15s/step - loss: 0.0020\n",
            "Epoch 81/100\n",
            "10/10 [==============================] - 130s 13s/step - loss: 0.0022\n",
            "Epoch 82/100\n",
            "10/10 [==============================] - 163s 16s/step - loss: 0.0020\n",
            "Epoch 83/100\n",
            "10/10 [==============================] - 138s 14s/step - loss: 0.0021\n",
            "Epoch 84/100\n",
            "10/10 [==============================] - 163s 17s/step - loss: 0.0023\n",
            "Epoch 85/100\n",
            "10/10 [==============================] - 156s 15s/step - loss: 0.0021\n",
            "Epoch 86/100\n",
            "10/10 [==============================] - 165s 17s/step - loss: 0.0020\n",
            "Epoch 87/100\n",
            "10/10 [==============================] - 180s 18s/step - loss: 0.0018\n",
            "Epoch 88/100\n",
            "10/10 [==============================] - 167s 16s/step - loss: 0.0019\n",
            "Epoch 89/100\n",
            "10/10 [==============================] - 197s 19s/step - loss: 0.0018\n",
            "Epoch 90/100\n",
            "10/10 [==============================] - 165s 17s/step - loss: 0.0021\n",
            "Epoch 91/100\n",
            "10/10 [==============================] - 165s 16s/step - loss: 0.0019\n",
            "Epoch 92/100\n",
            "10/10 [==============================] - 172s 18s/step - loss: 0.0018\n",
            "Epoch 93/100\n",
            "10/10 [==============================] - 180s 19s/step - loss: 0.0019\n",
            "Epoch 94/100\n",
            "10/10 [==============================] - 153s 15s/step - loss: 0.0017\n",
            "Epoch 95/100\n",
            "10/10 [==============================] - 146s 15s/step - loss: 0.0016\n",
            "Epoch 96/100\n",
            "10/10 [==============================] - 146s 14s/step - loss: 0.0016\n",
            "Epoch 97/100\n",
            "10/10 [==============================] - 146s 15s/step - loss: 0.0017\n",
            "Epoch 98/100\n",
            "10/10 [==============================] - 146s 15s/step - loss: 0.0016\n",
            "Epoch 99/100\n",
            "10/10 [==============================] - 137s 14s/step - loss: 0.0015\n",
            "Epoch 100/100\n",
            "10/10 [==============================] - 129s 13s/step - loss: 0.0016\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3ccfab4cd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "479DvEYdrX5v"
      },
      "source": [
        "# Make a prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_aH0CGdZoAk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aca296dc-481e-401a-cfac-06d6f7f94d07"
      },
      "source": [
        "color_me = []\n",
        "for filename in os.listdir('Test/'):\n",
        "    color_me.append(img_to_array(load_img('Test/'+filename)))\n",
        "color_me = np.array(color_me, dtype=float)\n",
        "color_me = 1.0/255*color_me\n",
        "color_me = gray2rgb(rgb2gray(color_me))\n",
        "color_me_embed = create_inception_embedding(color_me)\n",
        "color_me = rgb2lab(color_me)[:,:,:,0]\n",
        "color_me = color_me.reshape(color_me.shape+(1,))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py:3704: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable.debug_mode()`.\n",
            "  \"Even though the `tf.config.experimental_run_functions_eagerly` \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueFftUjKrgNV"
      },
      "source": [
        "# Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFERO_hPDyyV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d33b0bb-7399-495b-bf36-31a0a30a31e8"
      },
      "source": [
        "output = model.predict([color_me, color_me_embed])\n",
        "output = output * 128"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py:3704: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable.debug_mode()`.\n",
            "  \"Even though the `tf.config.experimental_run_functions_eagerly` \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-JQ-rEYrjmP"
      },
      "source": [
        "# Colorize the output of the test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6AEbAqrZ1El",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2d60843-fdbf-49d8-dec1-ec31e9feb96d"
      },
      "source": [
        "for i in range(len(output)):\n",
        "    cur = np.zeros((256, 256, 3))\n",
        "    cur[:,:,0] = color_me[i][:,:,0]\n",
        "    cur[:,:,1:] = output[i]\n",
        "    imsave(\"Result/img_\"+str(i)+\".png\", lab2rgb(cur))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}