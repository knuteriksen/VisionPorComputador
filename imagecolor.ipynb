{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imagecolor.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3acROV8DpFwz",
        "hEuR-tkzqJ5j",
        "479DvEYdrX5v",
        "ueFftUjKrgNV",
        "y-JQ-rEYrjmP"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNdqgd7mnYh1GYWuzVEAnXN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/knuteriksen/VisionPorComputador/blob/main/imagecolor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2erBPnfFYhLt"
      },
      "source": [
        "# Image Colorization\n",
        "\n",
        "Thanks to [Emil Wallner](https://blog.floydhub.com/colorizing-b-w-photos-with-neural-networks/) for inspiration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Jaf0C4RbZnx"
      },
      "source": [
        "# Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDteTNfeD_Oh"
      },
      "source": [
        "import keras\n",
        "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
        "from keras.preprocessing import image\n",
        "from keras.layers import Layer\n",
        "from keras.applications.inception_resnet_v2 import preprocess_input\n",
        "from keras.layers import Conv2D, UpSampling2D, InputLayer, Conv2DTranspose, Input, Reshape, merge, concatenate, Activation, Dense, Dropout, Flatten\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.callbacks import TensorBoard \n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers.core import RepeatVector, Permute\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\n",
        "from skimage.transform import resize\n",
        "from skimage.io import imsave\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jugVjBQvZGQ7"
      },
      "source": [
        "# Initialization\n",
        "Make sure that all keras stuff from previous sessions is cleared, and make sure we run in Eagerly mode. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iujoOigC5IEw"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "tf.config.run_functions_eagerly(True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5XACkmnbjZQ"
      },
      "source": [
        "# Obtaining images\n",
        "\n",
        "Get the training images from the local repository `Train/` and convert them to an array float type representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwDaIu5sY7JQ"
      },
      "source": [
        "# Get images\n",
        "X = []\n",
        "for filename in os.listdir('Train/'):\n",
        "    X.append(img_to_array(load_img('Train/'+filename)))\n",
        "X = np.array(X, dtype=float)\n",
        "Xtrain = 1.0/255*X"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPZmf5Etblru"
      },
      "source": [
        "# Downloading the Inception ResNet V2 with imagenet weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hry2uzQ5Y9JS"
      },
      "source": [
        "inception = InceptionResNetV2(weights='imagenet', include_top=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrAA3wJ0ch_3"
      },
      "source": [
        "# Create the Encoder\n",
        "\n",
        "Just a basic convoloutional encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9By_7TkeZSVs"
      },
      "source": [
        "encoder_input = Input(shape=(256, 256, 1,))\n",
        "encoder_output = Conv2D(64, (3,3), activation='relu', padding='same', strides=2)(encoder_input)\n",
        "encoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(encoder_output)\n",
        "encoder_output = Conv2D(128, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
        "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
        "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
        "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
        "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
        "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06EpjAKhckvO"
      },
      "source": [
        "# Create the fusion layer\n",
        "The fusion layer takes as input both the output of the encoder and the output of the inception layer.\n",
        "\n",
        "First Create an `Input` type. This variable will represent the output of the inception layer.\n",
        "\n",
        "Then:\n",
        "\n",
        "1.   Multiply the 1000 category Input layer `embed_input` (which contains the output of the inception layer) by 32*32 and store it in the `fusion_output` variable\n",
        "2.   Then Reshape the `fusion_output` layer\n",
        "3.   Then concatenate the fusion layer (now existing of the reshaped inception output) and the encoder output. **This is the actual fusion/concatination of the encoder and the inception.**\n",
        "4. Then do a 1x1 convolution to obtain a final size of 256@32x32 that we need for the decoder.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHbYdZVYZf-7"
      },
      "source": [
        "embed_input = Input(shape=(1000,))\n",
        "fusion_output = RepeatVector(32 * 32)(embed_input) \n",
        "fusion_output = Reshape(([32, 32, 1000]))(fusion_output)\n",
        "fusion_output = tf.keras.layers.Concatenate(axis=3)([encoder_output, fusion_output])\n",
        "fusion_output = Conv2D(256, (1, 1), activation='relu', padding='same')(fusion_output) "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kos5zpzcnTO"
      },
      "source": [
        "# Create the Decoder\n",
        "The Decoder takes the output of the fusion layer as input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9N-AboQZjKI"
      },
      "source": [
        "decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(fusion_output)\n",
        "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
        "decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\n",
        "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
        "decoder_output = Conv2D(32, (3,3), activation='relu', padding='same')(decoder_output)\n",
        "decoder_output = Conv2D(16, (3,3), activation='relu', padding='same')(decoder_output)\n",
        "decoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)\n",
        "decoder_output = UpSampling2D((2, 2))(decoder_output)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aE-ZCy-ic790"
      },
      "source": [
        "# Put together the whole model \n",
        "Remember that `embed_input` contains the output of the inception layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7kcRIXZZloJ"
      },
      "source": [
        "model = Model(inputs=[encoder_input, embed_input], outputs=decoder_output)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qmi2HxuudGv1"
      },
      "source": [
        "# Get inception output\n",
        "This output will be used as `embed_input` mentioned above.\n",
        "\n",
        "1.    First resize the grayscaled input images to fit into the inception model\n",
        "2.    Then use the preprocessor to feed the images in the correct format\n",
        "3.    Then we run the input through inception resnet v2 network and extracts the final layer of the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU3IZK19ZozB"
      },
      "source": [
        "def create_inception_embedding(grayscaled_rgb):\n",
        "    grayscaled_rgb_resized = []\n",
        "    for i in grayscaled_rgb:\n",
        "      i = resize(i, (299, 299, 3), mode='constant')\n",
        "      grayscaled_rgb_resized.append(i)\n",
        "    grayscaled_rgb_resized = np.array(grayscaled_rgb_resized)\n",
        "    grayscaled_rgb_resized = preprocess_input(grayscaled_rgb_resized)\n",
        "    embed = inception.predict(grayscaled_rgb_resized)\n",
        "    return embed"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3acROV8DpFwz"
      },
      "source": [
        "# Data Generator\n",
        "This is just  simple image data generator for training purposes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTfIGkJQZoew"
      },
      "source": [
        "datagen = ImageDataGenerator(\n",
        "        shear_range=0.4,\n",
        "        zoom_range=0.4,\n",
        "        rotation_range=40,\n",
        "        horizontal_flip=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEuR-tkzqJ5j"
      },
      "source": [
        "# Create Training Data\n",
        "This is the actual generator the backend uses to obtain the correct training data.\n",
        "\n",
        "1.   First we transform the rgb image to a grayscaled rgb\n",
        "2.   Then we use the `create_inception_embedding` funtion to obtain the `embedding_input` metnioned above. Again, this corresponds to the output of the inception layer.\n",
        "3.   Then we get the X and Y training data from the training dataset\n",
        "4.   Then we yield the input corresponding to the model created above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AnL6XTcZoWt"
      },
      "source": [
        "batch_size = 20\n",
        "\n",
        "def image_a_b_gen(batch_size):\n",
        "    for batch in datagen.flow(Xtrain, batch_size=batch_size):\n",
        "        grayscaled_rgb = gray2rgb(rgb2gray(batch))\n",
        "        embed = create_inception_embedding(grayscaled_rgb)\n",
        "        lab_batch = rgb2lab(batch)\n",
        "        X_batch = lab_batch[:,:,:,0]\n",
        "        X_batch = X_batch.reshape(X_batch.shape+(1,))\n",
        "        Y_batch = lab_batch[:,:,:,1:] / 128\n",
        "        yield ([X_batch, embed], Y_batch)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3T1z7LW1rTFe"
      },
      "source": [
        "# Compile and train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yP_mSlejZoMB",
        "outputId": "3c5a915d-6f6e-49f9-89fa-03da1317b605"
      },
      "source": [
        "tensorboard = TensorBoard(log_dir=\"/output\")\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.fit(image_a_b_gen(batch_size), callbacks=[tensorboard], epochs=10, steps_per_epoch=5)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py:3704: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable.debug_mode()`.\n",
            "  \"Even though the `tf.config.experimental_run_functions_eagerly` \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "5/5 [==============================] - 82s 17s/step - loss: 0.2169\n",
            "Epoch 2/10\n",
            "5/5 [==============================] - 90s 19s/step - loss: 0.0070\n",
            "Epoch 3/10\n",
            "5/5 [==============================] - 90s 17s/step - loss: 0.0062\n",
            "Epoch 4/10\n",
            "5/5 [==============================] - 98s 21s/step - loss: 0.0058\n",
            "Epoch 5/10\n",
            "5/5 [==============================] - 90s 17s/step - loss: 0.0055\n",
            "Epoch 6/10\n",
            "5/5 [==============================] - 90s 17s/step - loss: 0.0060\n",
            "Epoch 7/10\n",
            "5/5 [==============================] - 90s 19s/step - loss: 0.0058\n",
            "Epoch 8/10\n",
            "5/5 [==============================] - 74s 15s/step - loss: 0.0054\n",
            "Epoch 9/10\n",
            "5/5 [==============================] - 82s 17s/step - loss: 0.0057\n",
            "Epoch 10/10\n",
            "5/5 [==============================] - 106s 21s/step - loss: 0.0053\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3cdadd9e90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "479DvEYdrX5v"
      },
      "source": [
        "# Make a prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_aH0CGdZoAk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f4bc33a-731d-44c7-9260-4676ce890904"
      },
      "source": [
        "color_me = []\n",
        "for filename in os.listdir('Test/'):\n",
        "    color_me.append(img_to_array(load_img('Test/'+filename)))\n",
        "color_me = np.array(color_me, dtype=float)\n",
        "color_me = 1.0/255*color_me\n",
        "color_me = gray2rgb(rgb2gray(color_me))\n",
        "color_me_embed = create_inception_embedding(color_me)\n",
        "color_me = rgb2lab(color_me)[:,:,:,0]\n",
        "color_me = color_me.reshape(color_me.shape+(1,))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py:3704: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable.debug_mode()`.\n",
            "  \"Even though the `tf.config.experimental_run_functions_eagerly` \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueFftUjKrgNV"
      },
      "source": [
        "# Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFERO_hPDyyV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61a1573b-013e-4e8a-a6a2-e85461566b2e"
      },
      "source": [
        "output = model.predict([color_me, color_me_embed])\n",
        "output = output * 128"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py:3704: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable.debug_mode()`.\n",
            "  \"Even though the `tf.config.experimental_run_functions_eagerly` \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-JQ-rEYrjmP"
      },
      "source": [
        "# Colorize the output of the test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6AEbAqrZ1El",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beca185b-38f8-462d-f38d-22239ef6eff4"
      },
      "source": [
        "for i in range(len(output)):\n",
        "    cur = np.zeros((256, 256, 3))\n",
        "    cur[:,:,0] = color_me[i][:,:,0]\n",
        "    cur[:,:,1:] = output[i]\n",
        "    imsave(\"Result/img_\"+str(i)+\".png\", lab2rgb(cur))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}